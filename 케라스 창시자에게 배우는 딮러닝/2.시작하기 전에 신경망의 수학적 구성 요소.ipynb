{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 신경망과의 첫 만남\n",
    "\n",
    "1. 손글씨 숫자 분류를 학습하는 구체적인 신경망 예제 확인\n",
    "2. MNIST 흑백 손글씨 숫자 이미지 (28x28 픽셀)\n",
    "\n",
    "\n",
    "- MINIST 샘플 이미지\n",
    "![샘플_이미지](./images/MNIST_sample.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### keras를 이용한 MINST 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 이미지의 형상 : (60000, 28, 28)\n",
      "훈련 라벨의 형상 : (60000,)\n",
      "테스트 이미지의 형상 : (10000, 28, 28)\n",
      "테스트 라벨의 형상 : (10000,)\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "from keras.datasets import mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data() \n",
    "\n",
    "print(\"훈련 이미지의 형상 : {}\".format(train_images.shape))\n",
    "print(\"훈련 라벨의 형상 : {}\".format(train_labels.shape))\n",
    "print(\"테스트 이미지의 형상 : {}\".format(test_images.shape))\n",
    "print(\"테스트 라벨의 형상 : {}\".format(test_labels.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 변수 설명\n",
    "    1. *_images = 모델이 학습, 테스트해야 할 이미지\n",
    "    2. *_labels = 각 이미지들이 어떤 숫자인지를 알려주는 라벨(1차원 데이터)\n",
    "\n",
    "\n",
    "- 훈련 이미지는 총 6만장의 28 * 28 픽셀 이미지이다.\n",
    "- 테스트 이미지는 총 만장의 28 * 28 픽셀 이미지이다.\n",
    "\n",
    "\n",
    "- 이미지는 넘파이 배열로 인코딩되어 있으며, 레이블은 0~9까지의 숫자 배열이다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 신경망 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
    "network.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- layer\n",
    "    1. Dense 층 -> 각 노드가 완전연결 되어있는 층\n",
    "\n",
    "\n",
    "- 활성화 함수\n",
    "    1. relu -> 출력값이 0을 넘으면 출력값을 그대로 보내고, 0이하이면 0으로 보내는 함수\n",
    "    2. softmax -> 각 요소의 출력값을 0~1 사이의 값으로 내보내고, 총합이 1이 되게하는 함수(각 라벨에 대한 퍼센트를 나타내게 할 때 자주 사용한다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 훈련 준비를 마치기 위해 필요한 컴파일 단계\n",
    "    1. 손실 함수(loss function): 신경망의 성능을 측정, 네트워크가 학습할 방향을 정한다.\n",
    "    2. 옵티마이저(optimizer): 입력 데이터와 손실 함수를 사용하여 네트워크를 업데이트하는 방법\n",
    "    3. 훈련과 테스트 과정을 모니터링할 지표: 정확도, loss값등 훈련이 잘 되었는지 확인하는 용도로 사용 됨 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.compile(optimizer='rmsprop',            # mometum, Adagrad 같은 최적화 방법\n",
    "               loss='categorical_crossentropy', # 이진 문제가 아니기 때문에 categorical을 사용한다.\n",
    "               metrics=['accuracy'])            # 데이터의 라벨과 출력 데이터(예측 값)의 정확도를 측정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 입력 데이터 전처리\n",
    "    1. 입력 데이터를 0~1사이의 값으로 스케일 조정\n",
    "    2. 네트워크에 맞는 크기로 변경\n",
    "    \n",
    "- 레이블의 전처리\n",
    "    - (n,k) 크기의 2차원 배열로 변경 (n -> 훈련 데이터 개수,  k -> 클래스의 개수), one-hot 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 데이터 준비\n",
    "train_images = train_images.reshape((60000, 28*28))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "\n",
    "test_images = test_images.reshape((10000, 28*28))\n",
    "test_images = test_images.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기본 레이블 데이터 : 5\n",
      "전처리 한 레이블 데이터 : [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# 레이블 준비\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "print('기본 레이블 데이터 : {}'.format(train_labels[0]))\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "\n",
    "print('전처리 한 레이블 데이터 : {}'.format(train_labels[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 5s 86us/step - loss: 0.2530 - accuracy: 0.9263\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 5s 81us/step - loss: 0.1014 - accuracy: 0.9700\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 5s 80us/step - loss: 0.0666 - accuracy: 0.9804\n",
      "Epoch 4/5\n",
      "38528/60000 [==================>...........] - ETA: 1s - loss: 0.0485 - accuracy: 0.9849"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-417e80a50727>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 네트워크 학습\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\keras\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\keras\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\keras\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3725\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3726\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3727\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3729\u001b[0m     \u001b[1;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\keras\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1549\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1550\u001b[0m     \"\"\"\n\u001b[1;32m-> 1551\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1552\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1553\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\keras\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1589\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[0;32m   1590\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[1;32m-> 1591\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1593\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\keras\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\keras\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\keras\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 네트워크 학습\n",
    "network.fit(train_images, train_labels, epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 테스트 데이터를 이용한 학습된 네트워크 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터에 대한 평가\n",
    "test_loss, test_acc = network.evaluate(test_images, test_labels)\n",
    "print('테스트 데이터에 대한 평가 - 손실 함수 값: {}, 정확도 :{:%}'.format(test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 훈련 데이터에 대해서는 정확도가 98%인 반면, 테스트 데이터에 대해서는 97%의 정확도를 보인다. 이는 과대적합이 어느정도 일어났다는것을 의미한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 신경망을 위한 데이터 표현\n",
    "\n",
    "- 텐서\n",
    "    - 데이터를 위한 컨테이너\n",
    "    - 임의의 차원 개수를 가지는 행렬의 일반화된 모습\n",
    "    - 차원(dimension)을 축(axis)라고 부른다.\n",
    "\n",
    "## 2.1. 스칼라(0D 텐서)\n",
    "\n",
    "- 하나의 숫자만을 담고 있는 텐서\n",
    "- numpy에서는 float32, float64 타입의 숫자 스칼라 텐서\n",
    "- ndim을 이용하면 축의 개수를 확인할 수 있다.(스칼라 텐서의 축은 0개)\n",
    "- 텐서의 축 개수를 rank라고도 부른다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.array(12)\n",
    "print(\"x의 값 : {}\".format(x))\n",
    "print(\"x의 축 개수 : {}\".format(x.ndim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. 벡터(1D 텐서)\n",
    "\n",
    "- 숫자의 배열을 벡터 또는 1D 텐서라고 부른다.\n",
    "\n",
    "\n",
    "### 1D 텐서의 예"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([12, 3, 6, 14, 7])\n",
    "print(\"x의 값 : {}\".format(x))\n",
    "print(\"x의 축 개수 : {}\".format(x.ndim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위의 벡터는 5개의 원소를 가지므로 5D 벡터라고 부른다\n",
    "- 5D 벡터 -> 하나의 축으로 5개의 차원을 가짐\n",
    "- 5D 텐서 -> 5개의 축을 가짐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. 행렬(2D 텐서)\n",
    "\n",
    "- 벡터의 배열을 행렬 또는 2D 텐서라고 부른다.\n",
    "- 2개의 축이 존재한다.\n",
    "\n",
    "\n",
    "### 2D 텐서의 예"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[5, 78, 2, 34, 0],\n",
    "             [6,79, 3, 35, 1],\n",
    "             [7, 80, 4, 36, 2]])\n",
    "print(\"x의 값 : {}\".format(x))\n",
    "print(\"x의 축 개수 : {}\".format(x.ndim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 첫 번째 축을 행, 두 번째 축을 열이라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. 3D 텐서와 고차원 텐서\n",
    "\n",
    "- 행렬들을 하나의 배열로 합치면 3D 텐서가 된다.\n",
    "\n",
    "### 3D 텐서의 예"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[[5, 78, 2, 34, 0],\n",
    "              [6, 79, 3, 35, 1],\n",
    "              [7, 80, 4, 36, 2]],\n",
    "             [[5, 78, 2, 34, 0],\n",
    "             [6, 79, 3, 35, 1],\n",
    "             [7, 80, 4, 36, 2]],\n",
    "             [[5, 78, 2, 34, 0],\n",
    "             [6, 79, 3, 35, 1],\n",
    "             [7, 80, 4, 36, 2]]])\n",
    "print(\"x의 값 : {}\".format(x))\n",
    "print(\"x의 축 개수 : {}\".format(x.ndim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. 핵심 속성\n",
    "\n",
    "- 텐서는 다음과 같은 3개의 핵심 속성을 가지고 있다.\n",
    "    1. 축의 개수(rank) : 3D 텐서에는 3개의 축, 행렬에는 2개의 축이 있으며 numpy에서는 ndim 속성에 저장 되어있다.\n",
    "    2. 크기(shape) : 텐서의 각 축을 따라 얼마나 많은 차원이 있는지를 나타내는 튜플 ex) (3,5) -> 1축에 3개의 차원 2축에 5개의 차원\n",
    "    3. 데이터 타입 : 텐서에 포함된 데이터 타입 ex) float32, float64\n",
    "    \n",
    "    \n",
    "### MNIST 데이터로 텐서 속성 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()  # 데이터 로드\n",
    "\n",
    "print(\"훈련 이미지의 축 개수: {}\".format(train_images.ndim))\n",
    "print(\"훈련 이미지의 크기: {}\".format(train_images.shape))\n",
    "print(\"훈련 이미지의 데이터 타입 : {}\".format(train_images.dtype))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위 코드 출력으로 알 수 있는것\n",
    "    1. 정수형 8비트 3D 텐서이다.\n",
    "    2. 28 * 28 크기의 행렬이 6만개 있는 배열이다.\n",
    "    \n",
    "    \n",
    "### Matplotlib를 사용하여 데이터 확인하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit = train_images[4] # 훈련 이미지 중 5번째 이미지\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(digit, cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6. 넘파이로 텐서 조작하기\n",
    "\n",
    "- 위 코드에서 digit 같이 배열의 특정 원소를 선택하는 것을 슬라이싱(slicing)이라고 한다.\n",
    "\n",
    "\n",
    "### 슬라이스 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11 ~ 100번째를 슬라이스하여 배열을 만든다.\n",
    "\n",
    "my_slice = train_images[10:100]\n",
    "print(\"my_slice의 크기: {}\".format(my_slice.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_slice = train_images[10:100, :,:] # 이전과 동일하지만 2,3 번째 축의 범위도 설정함\n",
    "print(\"my_slice의 크기: {}\".format(my_slice.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_slice = train_images[10:100, 0:28,0:28] \n",
    "print(\"my_slice의 크기: {}\".format(my_slice.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지에서 오른쪽 아래 14 * 14 픽셀 범위만 슬라이스\n",
    "my_slice = train_images[10:100, 14:, 14: ] # : 다음에 아무 숫자도 쓰지 않으면 끝까지를 의미한다.\n",
    "print(\"my_slice의 크기: {}\".format(my_slice.shape))\n",
    "\n",
    "plt.imshow(my_slice[0], cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 음수 인덱스를 사용한 슬라이싱\n",
    "my_slice = train_images[:, 7:-7, 7:-7]\n",
    "print(\"my_slice의 크기: {}\".format(my_slice.shape))\n",
    "\n",
    "plt.imshow(my_slice[0], cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7. 배치 데이터\n",
    "\n",
    "- 일반적으로 딮러닝에서 텐서의 첫 번째 축은 샘플 수이다.\n",
    "- 전체 데이터를 나누어 작은 배치를 만든다.\n",
    "\n",
    "\n",
    "### 배치 데이터 생성 예"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = train_images[:128] # 0~127\n",
    "\n",
    "batch = train_images[128:256] # 128 ~ 255\n",
    "\n",
    "# n번째 배치 데이터 생성\n",
    "# batch = train_images[128 * n:128 * (n + 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이렇게 배치 데이터를 다룰 때 첫 번째 축을 배치 축이라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.8. 텐서의 실제 사례\n",
    "\n",
    "- 데이터의 종류\n",
    "    1. 벡터 데이터\n",
    "    2. 시계열 데이터\n",
    "    3. 이미지\n",
    "    4. 동영상"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 텐서 연산\n",
    "\n",
    "- 케라스에서 Dense 층 - keras.layers.Dense(512, activation='relu')\n",
    "- 위 함수를 풀었을 때 적용되는 연산\n",
    "    - output = relu(dot(W, input) + b)\n",
    "    - 위에서 쓰이는 연산\n",
    "        1. 점곱\n",
    "        2. 덧셈\n",
    "        3. relu (max(0, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. 원소별 연산\n",
    "\n",
    "- relu 함수와 덧셈은 원소별 연산이다.\n",
    "- 텐서에 있는 각 원소에 독립적으로 적용됨\n",
    "\n",
    "\n",
    "### 단순한 relu, 덧셈 연산 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def navie_relu(x):\n",
    "    assert len(x.shape) == 2\n",
    "    \n",
    "    x = x.copy()\n",
    "    \n",
    "    # 모든 x안의 원소를 돌면서 relu를 적용한다.\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i, j] = max(x[i,j], 0)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def navie_add(x, y):\n",
    "    assert len(x.shape) == 2\n",
    "    assert x.shape == y.shape\n",
    "    x = x.copy()\n",
    "    \n",
    "    # 모든 x안의 원소를 돌면서 뎃셈을 적용한다.\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i, j] += y[i, j]\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### numpy를 사용한 단순한 relu, 덧셈 연산 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# z = x + y 덧셈 연산\n",
    "# z = np.maximum(z, 0) ReLU 연산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. 브로드캐스팅\n",
    "\n",
    "- 크기가 다른 두 텐서가 더해질 때 사용 됨\n",
    "- 브로드캐스팅의 단계\n",
    "    1. 큰 텐서의 ndim에 맞도록 작은 텐서에 축이 추가\n",
    "    2. 작은 텐서가 새축을 따라서 큰 텐서의 크기에 맞도록 반복\n",
    "    \n",
    "\n",
    "### 구체적인 예\n",
    "1. x = (32, 10), y = (10,) 인 크기를 가진다고 가정\n",
    "2. y에 비어 있는 첫번째 축을 추가 y = (1, 10)이 됨\n",
    "3. y축에 2번을 32번 반복 y = (32, 10)으로 변경\n",
    "4. x와 y의 크기가 같아졌으므로 더할 수 있다.\n",
    "\n",
    "\n",
    "- 단 새로운 텐서가 만들어지는 것은 비효율적이므로 실제로 만들지는 않는다.\n",
    "\n",
    "\n",
    "### 코드 예"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_add_matrix_and_vector(x, y):\n",
    "    assert len(x.shape) == 2 # 2D 넘파이 배열\n",
    "    assert len(y.shape) == 1 # 넘파이 벡터\n",
    "    assert x.shape[1] == y.shape[0]\n",
    "    \n",
    "    x = x.copy()\n",
    "    \n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i,j] += y[j] # 이렇게 새로 축을 만들지는 않고 각 축에다 더해준다.\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. 텐서 점곱\n",
    "\n",
    "- numpy, 케라스에스는 dot 연산자를 사용\n",
    "- 원소의 개수가 같은 벡터끼리 점곱이 가능\n",
    "- 두 텐서 중 하나라도 ndim이 1보다 크면 dot 연산에 교환 법칙이 성립 되지 않는다.\n",
    "- (a,b) dot (b,c) = (a,c)로 크기가 변화된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. 텐서 크기 변환\n",
    "\n",
    "- 보통 전처리 할 때 사용\n",
    "- 특정 키기에 맞게 열과 행을 재배열"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. 텐서 연산의 기하학적 해석\n",
    "\n",
    "- 텐서의 내용은 기하학적 공간에 있는 좌표 포인트로 해석 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 신경망의 엔지: 그래디언트 기반 최적화\n",
    "\n",
    "- 이전에 보았던 신경망의 각 층은 입력 데이터를 다음과 같이 변환한다.\n",
    "   - output = relu(dot(W, input) * b)\n",
    "\n",
    "- 여기서 W, b는 층의 가중치, 편향이라고 부른다\n",
    "\n",
    "- 훈련이란 이 W, b를 반복하여 업데이트 하는것을 말한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras",
   "language": "python",
   "name": "keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
