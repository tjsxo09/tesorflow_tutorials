{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 머신 러닝의 네 가지 분류"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이전에 다루었던 문제들\n",
    "    1. 이진 분류 (리뷰 긍정, 부정 분류)\n",
    "    2. 다중 분류 (기사 토픽 분류)\n",
    "    3. 스칼라 회귀 (주택값 예측)\n",
    "\n",
    "- 위의 학습 방법들은 모두 지도 학습(supervised learning)의 예이다.\n",
    "\n",
    "- 지도 학습\n",
    "    - 훈련 데이터의 입력과 타깃 사이의 관계를 학습을 목적으로 학습하는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. 지도 학습\n",
    "\n",
    "- 가장 흔한 경우\n",
    "- 타깃에 입력 데이터를 매핑하는 방법을 학습\n",
    "\n",
    "\n",
    "- 특이한 경우\n",
    "    1. 시퀸스 생성 : 사진이 주어지면 이를 설명하는 캡션 생성\n",
    "    2. 구문 트리 예측 : 문장이 주어지면 분해된 구문 트리 예측\n",
    "    3. 물체 감지 : 사진이 주어지면 사진 안의 특정 물체 주위에 경계 상자를 그린다.\n",
    "    4. 이미지 분할 : 사진이 주어졌으 때 픽셀 단위로 특정 물체에 마스킹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. 비지도 학습\n",
    "\n",
    "- 어떤 타깃도 사용하지 않고 입력 데이터에 대한 변환을 찾는다.\n",
    "- 데이터 시각화, 데이터 압축, 데이터 노이즈 제거, 데이터의 상관관계를 알기 위해 사용\n",
    "- 데이터 분석에 중요한 요소이다.\n",
    "- 잘 알려진 것\n",
    "    1. 차원 축소\n",
    "    2. 군집(clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. 자기 지도 학습\n",
    "\n",
    "- 지도 학습의 특별한 경우\n",
    "- 사람이 만든 레이블을 사용하지 않는다. -> 사람이 개입하지 않는다.\n",
    "- 레이블은 필요하지만 보통 경험적인 알고리즘을 사용하여 데이터로부터 생성한다.\n",
    "- 오토 인코더가 잘 알려진 자기 지도 학습의 예이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. 강화 학습\n",
    "\n",
    "- 에이전트는 환경에 대한 정보를 받아 보상을 최대화하는 행동을 선택한다.\n",
    "- 대부분 연구 영역에 속함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 머신 러닝 모델 평가\n",
    "\n",
    "- 훈련에 사용한 데이터로 모델을 평가하지 않는 이유 : 훈련 데이터에 과대적합을 할 수 있기 때문\n",
    "- 머신 러닝의 목표 : 일반화된 모델을 얻는 것    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. 훈련, 검증, 테스트 셋\n",
    "\n",
    "- 훈련 셋과 테스트 셋 2개를 사용하지 않고 검증 셋을 추가해서 사용하는 이유\n",
    "    1. 모델을 개발할 때 모델의 설정을 튜닝하기 때문\n",
    "    2. 검증 셋의 성능을 기반으로 모델의 설정을 튜닝하면 검증 셋에 과대적합 할 수 있기 때문 -> 정보 누설\n",
    "    \n",
    "    \n",
    "- 데이터가 적을 때 훈련, 검증, 테스트 셋으로 나누는 고급 기법\n",
    "    1. 단순 홀드아웃 검증\n",
    "    2. K-겹 교차 검증\n",
    "    3. 셔플링을 사용한 반복 K-겹 교차 검증"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단순 홀드아웃 검증\n",
    "\n",
    "- 데이터의 일정량을 테스트 셋으로 떼어 놓는다.\n",
    "- 정보 누설을 막기 위해 테스트 셋을 사용하여 모델을 튜닝하는것은 안된다.\n",
    "\n",
    "\n",
    "- 단순 홀드아웃 검증 그림 예시\n",
    "![홀드_아웃_검증](./images/hold-out.png)\n",
    "\n",
    "\n",
    "### 홀드아웃 검증 구현 예"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nnum_validation_samples = 10000\\n\\nnp.random.shuffle(data) # 데이터를 무작위로 섞는것이 좋다. -> 오버 피팅 방지를 위해서\\n\\nvalidation_data = data[:num_validation_samples]\\ndata = data[num_validation_samples:]\\n\\ntraining_data = data[:]\\n\\nmodel = get_model()\\nmodel.train(training_data) # 훈련 데이터로 훈련 후 \\nvalidation_score = model.evaluate(validation_data) # 검증 데이터로 평가\\n\\n# 반복적으로 훈련 및 튜닝\\n\\nmodel = get_model()\\nmodel.train(np.concatenate([training_data, validation_data])) # 튜닝이 끝나면 전체 데이터를 가지고 훈련\\n\\ntest_score = model.evaluate(test_data)\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "num_validation_samples = 10000\n",
    "\n",
    "np.random.shuffle(data) # 데이터를 무작위로 섞는것이 좋다. -> 오버 피팅 방지를 위해서\n",
    "\n",
    "validation_data = data[:num_validation_samples]\n",
    "data = data[num_validation_samples:]\n",
    "\n",
    "training_data = data[:]\n",
    "\n",
    "model = get_model()\n",
    "model.train(training_data) # 훈련 데이터로 훈련 후 \n",
    "validation_score = model.evaluate(validation_data) # 검증 데이터로 평가\n",
    "\n",
    "# 반복적으로 훈련 및 튜닝\n",
    "\n",
    "model = get_model()\n",
    "model.train(np.concatenate([training_data, validation_data])) # 튜닝이 끝나면 전체 데이터를 가지고 훈련\n",
    "\n",
    "test_score = model.evaluate(test_data)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 단순 홀드아웃의 단점\n",
    "    1. 데이터가 적을 때는 검증 셋과 테스트 셋이 너무 적어 주어진 전체 데이터를 통계적으로 대표하지 못할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-겹 교차 검증\n",
    "\n",
    "- 데이터가 너무 적어 검증, 테스트 셋이 통계적으로 대표하지 못할 때 사용\n",
    "\n",
    "\n",
    "- 동일한 크기를 가진 K개 분할로 데이터를 나눈다.\n",
    "- 최종적으로는 K개의 점수 평균을 이용한다.\n",
    "\n",
    "\n",
    "- K-겹 교차 검증 사진 예시\n",
    "![kfold](./images/kfold.jpg)\n",
    "\n",
    "\n",
    "### K-겹 교차 검증 구현 예"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nk = 4\\nnum_validation_samples = len(data) // k\\n\\nnp.random.shuffle(data)\\n\\nvalidation_scores = []\\n\\nfor fold in range(k):\\n    validation_data = data[num_validation_samples * fold: num_validation_samples * (fold + 1)] # 검증 셋 선택\\n    training_data = data[:num_validation_samples * fold] + data[num_validation_samples * (fold + 1)] # np.concatenate로도 만들 수 있다.\\n    \\n    model = get_model()\\n    model.train(training_data)\\n    validation_score = model.evaluate(validation_data)\\n    validation_scores.append(validation_score)\\n    \\n    validation_scroe = np.average(validation_score)\\n    \\n    model = get_model()\\n    model.train(data)\\n    test_score = model.evaluate(test_data)\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "k = 4\n",
    "num_validation_samples = len(data) // k\n",
    "\n",
    "np.random.shuffle(data)\n",
    "\n",
    "validation_scores = []\n",
    "\n",
    "for fold in range(k):\n",
    "    validation_data = data[num_validation_samples * fold: num_validation_samples * (fold + 1)] # 검증 셋 선택\n",
    "    training_data = data[:num_validation_samples * fold] + data[num_validation_samples * (fold + 1)] # np.concatenate로도 만들 수 있다.\n",
    "    \n",
    "    model = get_model()\n",
    "    model.train(training_data)\n",
    "    validation_score = model.evaluate(validation_data)\n",
    "    validation_scores.append(validation_score)\n",
    "    \n",
    "    validation_scroe = np.average(validation_score)\n",
    "    \n",
    "    model = get_model()\n",
    "    model.train(data)\n",
    "    test_score = model.evaluate(test_data)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 셔플링을 사용한 K-겹 교차 검증\n",
    "\n",
    "- K 개로 분할을하기 전에 매번 데이터를 무작위로 섞는다.\n",
    "- 비용이 매우 많이 든다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. 기억해야 할 것\n",
    "\n",
    "- 평가 방식을 선택할 때\n",
    "    1. 대표성이 있는 데이터 : 훈련 셋과 테스트 셋은 데이터에 대한 대표성이 있어야한다.\n",
    "    2. 시간의 방향 : 과거에서 미래를 예측한다면 데이터를 무작위로 섞어서는 안 된다.\n",
    "    3. 데이터 중복 : 한 데이터셋에 어떤 데이터 포인트가 두 번 등장하면, 데이터를 섞고 훈련 셋과 검증 셋으로 나누었을 때 훈련 셋과 검증 셋에 데이터 포인트가 중복될 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 데이터 전처리, 특성 공학, 특성 학습\n",
    "\n",
    "\n",
    "## 3.1. 신경망을 위한 데이터 전처리\n",
    "\n",
    "- 전처리의 목적\n",
    "    - 원본 데이터를 신경망에 주입하기 쉽게 만드는것\n",
    "    \n",
    "- 전처리의 종류\n",
    "    1. 벡터화\n",
    "    2. 정규화\n",
    "    3. 누락된 값 다루기\n",
    "    4. 특성 추출\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 벡터화\n",
    "\n",
    "- 신경망의 입력과 타깃은 부동 소수 데이터로 이루어진 텐서여야 한다.\n",
    "- 입력 데이터를 텐서로 변화시키는 것을 데이터 벡터화라 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 값 정규화\n",
    "\n",
    "- 그레이스케일 인코딩의 경우 0~255 사이의 정수 값으로 인코딩 되어있다.\n",
    "- 위와 같은 값들을 255로 나누어서 0~1 상이의 부동 소수 값으로 만드는것을 값 정규화라고 한다.\n",
    "- 주택 가격 예측시 특성들의 범위가 제각각이어서 입력 값들을 독립적으로 정규화하여 평군이 0 표준편차가 1이되게 해주었다. 이도 값 정규화이다.\n",
    "\n",
    "\n",
    "- 네트워크를 쉽게 학습시키기 위해서 따라야하는 특징\n",
    "    1. 작은 값을 취한다.( 0~1 사이의 값을 가져야한다.)\n",
    "    2. 균일해야 한다. ( 모든 특성이 대체로 비슷한 범위를 가져야 한다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 누락된 값 다루기\n",
    "\n",
    "- 신경망에서 0에 할당된 값이 없다면 누락된 값을 0으로 설정하여도 좋다.\n",
    "- 테스트 데이터에 누락된 값이 포함될 가능성이 있다면, 훈련 데이터에도 누락된 값을 가지고 있어야한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. 특성 공학\n",
    "\n",
    "- 데이터와 머신 러닝 알고리즘에 관한 지식을 사용하는 단계\n",
    "- 모델에 데이터를 주입하기 전에 하드코딩된 변환을 적용하여 알고리즘이 더 잘 수행되도록 만들어 준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 과대적합과 과소적합\n",
    "\n",
    "- 훈련 데이터에 너무 적합하게 된 학습\n",
    "- 가장 좋은 해결 방법은 더 많은 학습 데이터를 모으는 것\n",
    "- 정보의 양을 조절, 저장할 수 있는 정보에 제약을 가하는것 -> 규제\n",
    "\n",
    "\n",
    "- 과소 적합\n",
    "    - 훈련 데이터의 손실이 낮을수록 테스트 데이터의 손실이 낮은 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. 네트워크 크기 축소\n",
    "\n",
    "- 과대적합을 막는 가장 단순한 방법 -> 모델의 크기, 학습 파라미터의 수를 줄이는 것\n",
    "- 손실을 최소화하기 위해 타깃에 대한 예측 성능을 가진 압축된 표현을 학습해야 한다.\n",
    "- 모델의 기억 용량이 너무 크지도 작지도 않은 부분을 찾아야한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 영화 리뷰 분류 모델 - 모델 크기를 다르게 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 17s 1us/step\n"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.datasets import imdb\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# 데이터 로드\n",
    "(train_data, train_targets), (test_data, test_targets) = imdb.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 데이터 벡터화\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    \n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1. # 특정 인덱스의 위치를 1로 만듬\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = vectorize_sequences(train_data)\n",
    "test_data = vectorize_sequences(test_data)\n",
    "\n",
    "# 타겟을 벡터로 변환\n",
    "train_targets = np.array(train_targets).astype('float32')\n",
    "test_targets = np.array(test_targets).astype('float32')\n",
    "\n",
    "# 검증 데이터 생성\n",
    "val_train_data = train_data[:10000]\n",
    "val_train_targets = train_targets[:10000]\n",
    "\n",
    "partial_train_data = train_data[10000:]\n",
    "partial_train_targets = train_targets[10000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 원본 모델과, 작은 모델 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def original_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "    model.add(layers.Dense(16, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer='rmsprop',\n",
    "                 loss = 'binary_crossentropy',\n",
    "                 metrics=['acc'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def small_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(6, activation='relu', input_shape=(10000,)))\n",
    "    model.add(layers.Dense(6, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer='rmsprop',\n",
    "                 loss = 'binary_crossentropy',\n",
    "                 metrics=['acc'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 각 모델의 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "15000/15000 [==============================] - 3s 171us/step - loss: 0.4941 - acc: 0.7949 - val_loss: 0.3624 - val_acc: 0.8735\n",
      "Epoch 2/20\n",
      "15000/15000 [==============================] - 2s 156us/step - loss: 0.2889 - acc: 0.9039 - val_loss: 0.2968 - val_acc: 0.8864\n",
      "Epoch 3/20\n",
      "15000/15000 [==============================] - 2s 154us/step - loss: 0.2096 - acc: 0.9319 - val_loss: 0.2774 - val_acc: 0.8903\n",
      "Epoch 4/20\n",
      "15000/15000 [==============================] - 2s 154us/step - loss: 0.1632 - acc: 0.9471 - val_loss: 0.2755 - val_acc: 0.8889\n",
      "Epoch 5/20\n",
      "15000/15000 [==============================] - 2s 154us/step - loss: 0.1356 - acc: 0.9549 - val_loss: 0.2927 - val_acc: 0.8867\n",
      "Epoch 6/20\n",
      "15000/15000 [==============================] - 2s 153us/step - loss: 0.1120 - acc: 0.9657 - val_loss: 0.3012 - val_acc: 0.8835\n",
      "Epoch 7/20\n",
      "15000/15000 [==============================] - 2s 154us/step - loss: 0.0895 - acc: 0.9741 - val_loss: 0.3194 - val_acc: 0.8835\n",
      "Epoch 8/20\n",
      "15000/15000 [==============================] - 2s 155us/step - loss: 0.0750 - acc: 0.9792 - val_loss: 0.3589 - val_acc: 0.8766\n",
      "Epoch 9/20\n",
      "15000/15000 [==============================] - 2s 155us/step - loss: 0.0575 - acc: 0.9861 - val_loss: 0.4161 - val_acc: 0.8656\n",
      "Epoch 10/20\n",
      "15000/15000 [==============================] - 2s 157us/step - loss: 0.0482 - acc: 0.9885 - val_loss: 0.3914 - val_acc: 0.8772\n",
      "Epoch 11/20\n",
      "15000/15000 [==============================] - 2s 157us/step - loss: 0.0391 - acc: 0.9918 - val_loss: 0.4616 - val_acc: 0.8680\n",
      "Epoch 12/20\n",
      "15000/15000 [==============================] - 2s 155us/step - loss: 0.0319 - acc: 0.9933 - val_loss: 0.4485 - val_acc: 0.8763\n",
      "Epoch 13/20\n",
      "15000/15000 [==============================] - 2s 157us/step - loss: 0.0254 - acc: 0.9950 - val_loss: 0.4746 - val_acc: 0.8746\n",
      "Epoch 14/20\n",
      "15000/15000 [==============================] - 2s 155us/step - loss: 0.0201 - acc: 0.9962 - val_loss: 0.5062 - val_acc: 0.8721\n",
      "Epoch 15/20\n",
      "15000/15000 [==============================] - 2s 156us/step - loss: 0.0171 - acc: 0.9969 - val_loss: 0.5346 - val_acc: 0.8712\n",
      "Epoch 16/20\n",
      "15000/15000 [==============================] - 2s 156us/step - loss: 0.0087 - acc: 0.9996 - val_loss: 0.5697 - val_acc: 0.8730\n",
      "Epoch 17/20\n",
      "15000/15000 [==============================] - 2s 154us/step - loss: 0.0104 - acc: 0.9983 - val_loss: 0.6181 - val_acc: 0.8646\n",
      "Epoch 18/20\n",
      "15000/15000 [==============================] - 2s 163us/step - loss: 0.0100 - acc: 0.9980 - val_loss: 0.6399 - val_acc: 0.8676\n",
      "Epoch 19/20\n",
      "15000/15000 [==============================] - 2s 159us/step - loss: 0.0036 - acc: 0.9999 - val_loss: 0.6670 - val_acc: 0.8671\n",
      "Epoch 20/20\n",
      "15000/15000 [==============================] - 2s 165us/step - loss: 0.0081 - acc: 0.9982 - val_loss: 0.7019 - val_acc: 0.8661\n"
     ]
    }
   ],
   "source": [
    "Omodel = original_model()\n",
    "\n",
    "O_history = Omodel.fit(partial_train_data, partial_train_targets,\n",
    "                    epochs=20,\n",
    "                    validation_data=(val_train_data, val_train_targets),\n",
    "                    batch_size = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "13312/15000 [=========================>....] - ETA: 0s - loss: 0.6404 - acc: 0.6369"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-cb891bea910d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_train_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_train_targets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m                     batch_size = 512)\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\keras\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\keras\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    184\u001b[0m                             fit_inputs[:-1], batch_ids) + [fit_inputs[-1]]\n\u001b[0;32m    185\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 186\u001b[1;33m                         \u001b[0mins_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslice_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfit_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    187\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m                     raise TypeError('TypeError while preparing batch. '\n",
      "\u001b[1;32m~\\anaconda3\\envs\\keras\\lib\\site-packages\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mslice_arrays\u001b[1;34m(arrays, start, stop)\u001b[0m\n\u001b[0;32m    553\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'shape'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    556\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\keras\\lib\\site-packages\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    553\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'shape'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    556\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Smodel = small_model()\n",
    "\n",
    "S_history = Smodel.fit(partial_train_data, partial_train_targets,\n",
    "                    epochs=20,\n",
    "                    validation_data=(val_train_data, val_train_targets),\n",
    "                    batch_size = 512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 검증 손실 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "original_val_loss = O_history.history['val_loss']\n",
    "smaller_val_loss = S_history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(original_val_loss) + 1)\n",
    "\n",
    "plt.plot(epochs, original_val_loss, 'b+', label='original model') # x축에 epochs 값으로, y축을 loss 값으로 설정\n",
    "plt.plot(epochs, smaller_val_loss, 'bo', label='smaller model')\n",
    "plt.xlabel('Epochs') # 축 라벨을 Epochs로 설정\n",
    "plt.ylabel('Validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 기존 네트워크보다 작은 모델이 더 느게 과대 적합 된다.\n",
    "\n",
    "\n",
    "### 더 큰 모델 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigger_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(1024, activation='relu', input_shape=(10000,)))\n",
    "    model.add(layers.Dense(1024, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer='rmsprop',\n",
    "                 loss = 'binary_crossentropy',\n",
    "                 metrics=['acc'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bmodel = bigger_model()\n",
    "\n",
    "B_history = Bmodel.fit(partial_train_data, partial_train_targets,\n",
    "                    epochs=20,\n",
    "                    validation_data=(val_train_data, val_train_targets),\n",
    "                    batch_size = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "\n",
    "original_val_loss = O_history.history['val_loss']\n",
    "bigger_val_loss = B_history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(original_val_loss) + 1)\n",
    "\n",
    "plt.plot(epochs, original_val_loss, 'b+', label='original model') # x축에 epochs 값으로, y축을 loss 값으로 설정\n",
    "plt.plot(epochs, bigger_val_loss, 'bo', label='bigger model')\n",
    "plt.xlabel('Epochs') # 축 라벨을 Epochs로 설정\n",
    "plt.ylabel('Validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 큰 모델이 더욱 빠르게 과대적합하는것을 볼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. 가중치 규제 추가\n",
    "\n",
    "- 오캄의 면도날\n",
    "    - 어떤 것에 대한 두 가지의 설명이 있다면 더  적은 가정이 필요한 간단한 설명이 옳다.\n",
    "    \n",
    "- 간단한 모델이 복잡한 모델보다 덜 과대적합될 가능성이 높다.\n",
    "\n",
    "\n",
    "- 가중치 규제\n",
    "    - 네트워크 복잡도에 제한을 두어 가중치가 작은 값을 가지도록 강제하는 것\n",
    "    \n",
    "    \n",
    "- 가중치 규제의 종류\n",
    "    1. L1 규제 : 가중치의 절대값에 비례하는 비용이 추가\n",
    "    2. L2 규제 : 가중치의 제곱에 비례하는 비용이 추가 ( 가중치 감쇠라고도 불림)\n",
    "    \n",
    "\n",
    "### 모델에 L2 가중치 추가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001),\n",
    "                       activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001),\n",
    "                      activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- l2(0.001)은 가중치 행렬의 모든 원소를 제곱하고 0.001을 곱하여 네트워크의 전체 손실에 더해진다는 의미이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "                 loss = 'binary_crossentropy',\n",
    "                 metrics=['acc'])\n",
    "\n",
    "L2_history = model.fit(partial_train_data, partial_train_targets,\n",
    "                    epochs=20,\n",
    "                    validation_data=(val_train_data, val_train_targets),\n",
    "                    batch_size = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "\n",
    "original_val_loss = O_history.history['val_loss']\n",
    "L2_val_loss = L2_history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(original_val_loss) + 1)\n",
    "\n",
    "plt.plot(epochs, original_val_loss, 'b+', label='original model') # x축에 epochs 값으로, y축을 loss 값으로 설정\n",
    "plt.plot(epochs, L2_val_loss, 'bo', label='L2-regularized model')\n",
    "plt.xlabel('Epochs') # 축 라벨을 Epochs로 설정\n",
    "plt.ylabel('Validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 원본 모델보다 가중치 규제를 사용한 모델이 과대적합에 잘 견디는것을 볼 수 있다.\n",
    "- L2 규제 말고도 L1 규제, L1_L2 규제 병행을 사용 할 수도 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. 드롭아웃 추가\n",
    "\n",
    "- 신경망을 위해 사용되는 규제 기법 중에서 가장 효과적이고 널리 사용되는 방법 중 하나\n",
    "- 훈련하는 도안 무작위로 층의 일부 출력 특성을 제외시킨다.\n",
    "- 뉴런들의 부정한 협업을 방지하고 과대적합을 방해한다.\n",
    "- 중요하지 않은 우연한 패턴을 깨트린다.\n",
    "\n",
    "\n",
    "### IMDB 네트워크에 드롭아웃 추가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dropout(0.5)) # 50%를 무시하겠다는 의미\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "                 loss = 'binary_crossentropy',\n",
    "                 metrics=['acc'])\n",
    "\n",
    "Droout_history = model.fit(partial_train_data, partial_train_targets,\n",
    "                    epochs=20,\n",
    "                    validation_data=(val_train_data, val_train_targets),\n",
    "                    batch_size = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "\n",
    "original_val_loss = O_history.history['val_loss']\n",
    "D_val_loss = Droout_history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(original_val_loss) + 1)\n",
    "\n",
    "plt.plot(epochs, original_val_loss, 'b+', label='original model') # x축에 epochs 값으로, y축을 loss 값으로 설정\n",
    "plt.plot(epochs, D_val_loss, 'bo', label='Dropout model')\n",
    "plt.xlabel('Epochs') # 축 라벨을 Epochs로 설정\n",
    "plt.ylabel('Validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 기본 네트워크보다 성능이 더 향상된 것을 볼 수 있다.\n",
    "\n",
    "\n",
    "### 과대적합을 방지하기 위해서 사용되는 방법들\n",
    "1. 훈련 데이터를 더 모은다.\n",
    "2. 네트워크 용량을 감소시킨다.\n",
    "3. 가중치 규제를 추가한다.\n",
    "4. 드롭아웃을 추가한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 보편적인 머신 러닝 작업 흐름\n",
    "\n",
    "\n",
    "## 5.1. 문제 정의와 데이터셋 수집\n",
    "\n",
    "- 주어진 문제 정의\n",
    "    1. 가용데이터의 유무\n",
    "    2. 문제의 유형(이진 분류, 다중 분류, 회귀, 군집....)\n",
    "    \n",
    "    \n",
    "- 입력과 출력\n",
    "    1. 주어진 입력으로 출력을 예측할 수 있다고 가설을 세움\n",
    "    2. 가용한 데이터에 입력과 출력 사이의 관계를 학습하는 데 충분한 정보가 있다고 가설을 세운다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. 성공 지표 선택\n",
    "\n",
    "- 클래스 분포가 균일한 분류 문제에서는 정확도, ROC AUC가 일반적인 지표\n",
    "- 랭킹 문제나 다중 레이블 문제에서는 평균 정밀도를 사용할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. 평가 방법 선택\n",
    "\n",
    "- 앞서 잘 알려진 세 가지의 평가 방식이 있다.\n",
    "    1. 홀드아웃 검증 셋 분리\n",
    "    2. K-겹 교차 검증\n",
    "    3. 반복 K-겹 교차 검증\n",
    " \n",
    "- 대부분의 경우 첫 번째로 충분하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4. 데이터 준비\n",
    "\n",
    "- 머신 러닝 모델을 심층 신경망이라고 가정\n",
    "- 데이터는 텐서로 구성\n",
    "- 텐서는 일반적으로 작은 값 (0~1) 혹은 (-1,1) 범위로 되어있다.\n",
    "- 특성마다 범위가 다르면 정규화를 한다.\n",
    "- 특성 공학을 수행 할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5. 기본보다 나은 모델 훈련하기\n",
    "\n",
    "- 이 단계의 목표\n",
    "    - 통계적 검정력을 달성하는 것(아주 단순한 모델보다 나은 수준의 작은 모델을 개발)\n",
    "\n",
    "\n",
    "- 통계적 검정력을 달성하는 것이 항상 가능하지는 않다.\n",
    "    - 여러개의 타당성 있는 네트워크 구조를 시도해 보고 무작위로 예측하는 모델보다 낫지 않다면 문제 정의에서 한 2개의 가설이 틀렸을 수 있다.\n",
    "    - 가설이 틀렸을 경우 기획부터 다시 해야 한다.\n",
    "    \n",
    "    \n",
    "- 일이 잘 진행되었다면 모델을 만들기 위해 세 가지 중요한 선택을 한다.\n",
    "    1. 마지막 층의 활성화 함수 : 네트워크 출력에 필요한 제한을 가한다.\n",
    "    2. 손실 함수 : 풀려고 하는 문제의 종류에 적합해야 한다.(이진 분류:binary_crossentropy, 회귀: mse)\n",
    "    3. 최적화 설정: 어떤 옵티마이저를 사용하는가?, 학습률은 어떻게 설정하였는가를 생각한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|문제 유형|마지막 층의 활성화 함수|손실 함수|\n",
    "|:---:|:---:|:---:|\n",
    "|이진 분류|시그모이드|binary_crossentropy|\n",
    "|단일 레이블 다중 분류|소프트맥스|categorical_crossentropy|\n",
    "|다중 레이블 다중 분류|시그모이드|binary_crossentropy|\n",
    "|임의 값에 대한 회귀|없음|mse|\n",
    "|0과 1 사이 값에 대한 회귀|시그모이드|mse 또는 binary_crossentropy|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6. 몸집 키우기: 과대적합 모델 구축\n",
    "\n",
    "- 통계적 검정력을 가진 모델을 얻음 -> 충분히 성능을 내는지 확인\n",
    "- 또한 적절한 용량을 찾기 위해\n",
    "    1. 층을 추가\n",
    "    2. 층의 크기를 키움\n",
    "    3. 더 많은 에포크로 훈련\n",
    "- 위 과정을 이용해 과대적합한 모델을 만들고 용량을 줄여나간다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7. 모델 규제와 하이퍼파라미터 튜닝\n",
    "\n",
    "- 이 단계가 대부분의 시간을 차지\n",
    "- 모델을 수정할 때 적용해 볼 것들\n",
    "    1. 드롭아웃 추가\n",
    "    2. 층을 추가하거나 제거, 다른 구조 시도\n",
    "    3. L1, L2 또는 두가지 모두 추가\n",
    "    4. 최적의 설정을 찾기 위해 하이퍼파리미터를 바꿈\n",
    "    5. 선택적으로 특성 공학을 시도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras",
   "language": "python",
   "name": "keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
